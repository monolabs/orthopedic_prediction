---
title: "Orthopedic Condition Prediction"
author: "Steven Pramono"
date: "12/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Project overview

This project aims to analyze biomechanical features of orthopedic patients and use them to predict patients' conditions.
Two datasets were analyzed in this project: "column_2C_weka.csv" and "column_3C_weka.csv". The first classifies diagnostic outcomes into two categories: "Abnormal" and "Normal" while the second further divides abnormality into "Disk Hernia" and "Spondylolisthesis". The datasets are otherwise identical in structure. Both data sets will be referred to in this report as **Data 1** and **Data 2** respectively.

Note: bin_class column is added where "Abnormal" is represented by 0 and "Normal" by 1.

```{r setting up datasets, message=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(Matrix)) install.packages("Matrix", repos = "http://cran.us.r-project.org")

col_names=c("pelvic_incidence","pelvic_tilt","lumbar_lordosis_angle","sacral_slope","pelvic_radius","degree_spondylolisthesis","class")
dat1<-read_csv("../data/column_2C_weka.csv")%>%setNames(col_names)%>%na.omit%>% mutate(class=factor(class,levels=unique(class)))
dat2<-read_csv("../data/column_3C_weka.csv")%>%setNames(col_names)%>%na.omit%>%mutate(class=factor(class,levels=unique(class)))
  
```

We start with **Data 1**, here is the first few lines of the data:

```{r data viewing}
head(dat1)
```

Structure of data analysis is as follows:

* Datasets exploration
    + Data visualization and structure
* Models fitting
    + Logistic regression
    + K-nearest neighbors
    + random forest
* Analysis and optimization


# 2. Datasets exploration

## 2.1. Visualizing the data

We start by plotting the distribution of each variables in dat1:

```{r variability of each predictors}
dat1%>%gather(type,value,-class)%>%
  ggplot(aes(type,value,color=class))+
    geom_jitter(alpha=0.2)+
    theme(axis.text.x=element_text(angle=45,hjust=1))
```

There is a clear outlier at degree_spondylolisthesis >400 possibly due to error. Therefore, we exclude that particular observation from analysis:

```{r outlier removal, echo=FALSE}
dat1<-dat1%>%filter(degree_spondylolisthesis<400)
dat1%>%gather(type,value,-class)%>%
  ggplot(aes(type,value,color=class))+
    geom_jitter(alpha=0.2)+
    theme(axis.text.x=element_text(angle=45,hjust=1))
```

Spondylolisthesis degree seems to have the highest predictive power: abnormality is expected from high degree. With histograms, it is easier to analyze the type of distributions each of the predictors has:

```{r predictors histograms, fig.width=12, fig.height=4}
dat1%>%gather(type,value,-class)%>%
  ggplot(aes(value))+
    geom_histogram(binwidth=5)+
    facet_grid(.~type)
```

We can see that distributions of degree_spondylolisthesis is the only one not well approximated by normal distribution.

## 2.2. Data structure

Correlation matrix can be constructed to observe dependencies between predictors:

```{r correlation}
cor1<-cor(dat1[1:(length(dat1)-1)])
cor1
corrplot(cor1)
```

The high correlations between predictors is obvious:

```{r correlation plots, fig.width=12, fig.height=4}
cor_plot_1<-dat1%>%
  ggplot(aes(sacral_slope,pelvic_incidence,color=class))+
    geom_point(alpha=0.4)
cor_plot_2<-dat1%>%
  ggplot(aes(lumbar_lordosis_angle,pelvic_incidence,color=class))+
    geom_point(alpha=0.4)
cor_plot_3<-dat1%>%
  ggplot(aes(lumbar_lordosis_angle,degree_spondylolisthesis,color=class))+
    geom_point(alpha=0.4)
grid.arrange(cor_plot_1,cor_plot_2,cor_plot_3,nrow=1)
```

```{r correlation plots with ranks, fig.width=12, fig.height=4, echo=FALSE}
cor_plot_1<-dat1%>%
  ggplot(aes(rank(sacral_slope),rank(pelvic_incidence),color=class))+
    geom_point(alpha=0.4)
cor_plot_2<-dat1%>%
  ggplot(aes(rank(lumbar_lordosis_angle),rank(pelvic_incidence),color=class))+
    geom_point(alpha=0.4)
cor_plot_3<-dat1%>%
  ggplot(aes(rank(lumbar_lordosis_angle),rank(degree_spondylolisthesis),color=class))+
    geom_point(alpha=0.4)
grid.arrange(cor_plot_1,cor_plot_2,cor_plot_3,nrow=1)
```


# 3. Models fitting

In this section, several fitting models were explored and compared. Here is the list of algorithms discussed:

LIST

Evaluation was done using different metrics such as accuracy, precision, F1 score (collectively referred to as **results**). Tuning parameters are optimized whenever applicable. Result from optimized model in each method applied to a test set is documented in a table.

Default bootstrap samples proportions and numbers of bootstrap sets are used.Test set is comprised of 20% observations, randomly selected using Monte Carlo simulation:

```{r include=FALSE}
set.seed(2346,sample.kind="Rounding")
```
```{r partitioning for Data 1}
test_index1<-createDataPartition(dat1$class,times=1,p=0.2)
test_set1<-dat1[unlist(test_index1),]
train_set1<-dat1[-unlist(test_index1),]
```
```{r include=FALSE}
set.seed(2346,sample.kind="Rounding")
```
```{r partitioning for Data 2}
test_index2<-createDataPartition(dat2$class,times=1,p=0.2)
test_set2<-dat2[unlist(test_index2),]
train_set2<-dat2[-unlist(test_index2),]
```

and prevalence of abnormality in **train_set1** and **train_set2** is approximately 3:

```{r prevalence in train_sets}
train_set1%>%group_by(class)%>%summarize(n=n())
train_set2%>%group_by(class)%>%summarize(n=n())
```

General function to return accuracies, precision, recall and F1 score are defined with "Abnormal" or "0" as the positive. In this case, it is important to maximize recall to catch as much abnormality as possible.

```{r evaluation functions}
#results function for Data 1 - returns a list of accuracy,recall,precision and F1
results_func<-function(predictions,tests){ #predictions and tests are both vectors
  cf<-confusionMatrix(data=predictions,reference=tests)
  c(cf$overall['Accuracy'],cf$byClass['Sensitivity'],cf$byClass['Precision'],cf$byClass['F1'])
}
#results function for Data 2 - returns a list of (in order): accuracy, recall(Her),precision(Her), F1(Her),recall(Spo),precision(Spo),F1(Spo)
results_func2<-function(predictions,tests){ #predictions and tests are both vectors
  cf<-confusionMatrix(data=predictions,reference=tests)
  c(cf$overall['Accuracy'],cf$byClass[1,1],cf$byClass[1,2],cf$byClass[1,7],cf$byClass[2,1],cf$byClass[2,2],cf$byClass[2,7])
}
```

## 3.1. Benchmark

Before exploring different machine learning algorithms, we can make use of the high predicting power of one of the predictors, degree_spondylolisthesis to construct a rather simple prediction. For Data 1, we can predict high degree_spondylolisthesis with "Abnormal". Here we experimented with cutoff values from -12 to 25.

```{r benchmark Data 1}
cut_dp<-seq(-12,25,0.1)
predict_benchmark<-function(cutoff){ #data is a vector of degree_spondylolisthesis
  mean(ifelse(train_set1$degree_spondylolisthesis>cutoff,"Abnormal","Normal")==train_set1$class)
}
tibble(cutoff=cut_dp, train_accuracy=sapply(cut_dp,predict_benchmark))%>%arrange(desc(train_accuracy))

```

For simplicity, only cutoff value of 5.4 is used to evaluate test set corresponding to Data 1. Here is the result:

```{r result_benchmark}
predictions_benchmark1<-factor(ifelse(test_set1$degree_spondylolisthesis>5.4,"Abnormal","Normal"),levels=c("Abnormal","Normal"))
results_benchmark1<-results_func(predictions_benchmark1,test_set1$class)
tab_benchmark1<-tibble(data="Data 1",model="benchmark",accuracy=results_benchmark1[1],recall=results_benchmark1[2],precision=results_benchmark1[3],F1=results_benchmark1[4])
tab_benchmark1
```

Benchmarking for Data 2 requires more analysis and deeper domain expertise. Without in-depth knowledge on abnormalities, we can perhaps still construct a benchmark for Data 2 by randomly predicting "Hernia" or "Spondylolisthesis" for each observation falling under "Abnormal" class. However, the accuracy of such benchmark will be noticeably lower than in Data 1's case.


## 3.2. Logistic regression

Logistic regression is commonly used when the outcome is binary. To train a model using all predictors, we can use the following code:

```{r glm prediction}
fit_glm<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="glm",data=train_set1,family="binomial")
predictions_glm<-predict(fit_glm,newdata=test_set1,type="raw")
results_glm<-results_func(predictions_glm,test_set1$class)
```

We see an error stating probabilities of 0 and 1 are observed in the model fit_glm. Viewing the predictions in descending order of probability_0 helps visualizing the problem.

```{r glm prediction by probability}
predict(fit_glm,newdata=test_set1,type="prob")%>%arrange(desc(Abnormal))%>%head
```

Some of the values of probability_0 are 1 or really close to 1 (or close to 0 for probability_1). Earlier we Restablished that degree_spondylolisthesis of >50 seems to result in 100% probability of being abnormal. Therefore, we can exclude the corresponding observations to get rid of the error.

```{r excluding data with degree_spondylolisthesis >50}
fit_glm2<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="glm",data=train_set1%>%filter(degree_spondylolisthesis<=50),family="binomial")
predictions_glm2<-predict(fit_glm2,newdata=test_set1,type="raw")
results_glm2<-results_func(predictions_glm2,test_set1$class)
```

Note than the resulting model after exclusion seems to give identical prediction.

```{r predictions_glm vs predictions_glm2}
identical(predictions_glm,predictions_glm2)
```

Here is the results:

```{r result_glm, echo=FALSE}
tab_glm<-tibble(data="Data 1",model="glm",accuracy=results_glm2[1],recall=results_glm2[2],precision=results_glm2[3],F1=results_glm2[4])
tab_glm
```


## 3.3. K-nearest neighbors (knn)

### 3.3.1 Data 1 - Bootstrap

Here we will train knn models for different values of k (tuneLength = 20) and evaluate their performances on the test set. The plot and training results are summarized below.

```{r, include=FALSE}
set.seed(1234,sample.kind="Rounding")
```
```{r knn_bootstrap_Data 1}
fit_knn<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="knn",data=train_set1,tuneLength=20)
predictions_knn<-predict(fit_knn,newdata=test_set1,type="raw")
results_knn<-results_func(predictions_knn,test_set1$class)
fit_knn$results%>%arrange(desc(Accuracy))%>%head
plot(fit_knn)
```

Results for **Data 1** with **knn** and **default bootstrap**:

```{r results_knn_bootstrap_Data 1, echo=FALSE}
tab_knn_boot1<-tibble(data="Data 1",model="knn_bootstrap",accuracy=results_knn[1],recall=results_knn[2],precision=results_knn[3],F1=results_knn[4])
tab_knn_boot1
```

### 3.3.2. Data 1 - 10-fold cross validation

```{r, include=FALSE}
set.seed(3573,sample.kind="Rounding")
```
```{r knn_cv_Data 1}
fit_knn_cv1<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="knn",data=train_set1,tuneLength=20,trControl=(trainControl(method="cv",number=10)))
predictions_knn_cv1<-predict(fit_knn_cv1,newdata=test_set1,type="raw")
results_knn_cv1<-results_func(predictions_knn_cv1,test_set1$class)
fit_knn_cv1$results%>%arrange(desc(Accuracy))%>%head
plot(fit_knn_cv1)
```

Results for **Data 1** with **knn** and **10-fold cv**:

```{r results_knn_cv_Data 1, echo=FALSE}
tab_knn_cv1<-tibble(data="Data 1",model="knn_cv",accuracy=results_knn_cv1[1],recall=results_knn_cv1[2],precision=results_knn_cv1[3],F1=results_knn_cv1[4])
tab_knn_cv1
```

### 3.3.3. Data 2 - Bootstrap

Knn is supports non-linearity and allows for classification into more than 2 classes. Here is the same procedure done on the second data set with 3 classes of outcome.

```{r, include=FALSE}
set.seed(4365,sample.kind="Rounding")
```
```{r knn_bootstrap_Data 2}
fit_knn2<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="knn",data=train_set2,tuneLength=20)
predictions_knn2<-predict(fit_knn2,newdata=test_set2,type="raw")
results_knn2<-results_func2(predictions_knn2,test_set2$class)
fit_knn2$results%>%arrange(desc(Accuracy))%>%head
plot(fit_knn2)
```

Results for **Data 2** with **knn** and **bootstrap**:

```{r result_knn_bootstrap_Data 2, echo=FALSE}
tab_knn_boot2<-tibble(data="Data 2",model="knn_bootstrap",accuracy=results_knn2[1],recall_hernia=results_knn2[2],precision_hernia=results_knn2[3],F1_hernia=results_knn2[4],recall_spond=results_knn2[5],precision_spond=results_knn2[6],F1_spond=results_knn2[7])
tab_knn_boot2
```

### 3.3.4. Data 2 - 10-fold cross validation

```{r, include=FALSE}
set.seed(4589,sample.kind="Rounding")
```
```{r knn_cv_Data 2}
fit_knn_cv2<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="knn",data=train_set2,tuneLength=20,trControl=(trainControl(method="cv",number=10)))
predictions_knn_cv2<-predict(fit_knn_cv2,newdata=test_set2,type="raw")
results_knn_cv2<-results_func2(predictions_knn_cv2,test_set2$class)
fit_knn_cv2$results%>%arrange(desc(Accuracy))%>%head
plot(fit_knn_cv2)
```

```{r results_knn_cv_Data 2, echo=FALSE}
tab_knn_cv2<-tibble(data="Data 2",model="knn_cv",accuracy=results_knn_cv2[1],recall_hernia=results_knn_cv2[2],precision_hernia=results_knn_cv2[3],F1_hernia=results_knn_cv2[4],recall_spond=results_knn_cv2[5],precision_spond=results_knn_cv2[6],F1_spond=results_knn_cv2[7])
tab_knn_cv2
```

## 3.4. Random forest

### 3.4.1. Data 1 - Bootstrap

```{r, include=FALSE}
set.seed(3456,sample.kind="Rounding")
```
```{r rf_bootstrap_Data 1}
fit_rf<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="rf",data=train_set1,tuneLength=5)
predictions_rf<-predict(fit_rf,newdata=test_set1,type="raw")
results_rf<-results_func(predictions_rf,test_set1$class)
fit_rf$results%>%arrange(desc(Accuracy))%>%head
plot(fit_rf)
```

Results for **Data 1** with **rf** and **default bootstrap**:

```{r variable_importance_rf_bootstrap_Data 1}
varImp(fit_rf)
```

```{r result_rf_bootstrap_Data 1, echo=FALSE}
tab_rf_boot1<-tibble(data="Data 1",model="rf_bootstrap",accuracy=results_rf[1],recall=results_rf[2],precision=results_rf[3],F1=results_rf[4])
tab_rf_boot1
```

### 3.4.2. Data 1 - 10-fold cross validation

```{r, include=FALSE}
set.seed(1296,sample.kind="Rounding")
```
```{r rf_cv_Data 1}
fit_rf_cv1<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="rf",data=train_set1,tuneLength=5,trControl=trainControl(method="cv",number=10))
predictions_rf_cv1<-predict(fit_rf_cv1,newdata=test_set1,type="raw")
results_rf_cv1<-results_func(predictions_rf_cv1,test_set1$class)
fit_rf$results%>%arrange(desc(Accuracy))%>%head
plot(fit_rf_cv1)
```

Results for **Data 1** with **rf** and **10-fold cross validation**:

```{r variable_importance_rf_cv_Data 1}
varImp(fit_rf_cv1)
```

```{r results_rf_cv_Data 1, echo=FALSE}
tab_rf_cv1<-tibble(data="Data 1",model="rf_cv",accuracy=results_rf_cv1[1],recall=results_rf_cv1[2],precision=results_rf_cv1[3],F1=results_rf_cv1[4])
tab_rf_cv1
```

### 3.4.3. Data 2 - Bootstrap

```{r, include=FALSE}
set.seed(3498,sample.kind="Rounding")
```
```{r rf_bootstrap_Data 2}
fit_rf2<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="rf",data=train_set2,tuneLength=5)
predictions_rf2<-predict(fit_rf2,newdata=test_set2,type="raw")
results_rf2<-results_func2(predictions_rf2,test_set2$class)
fit_rf2$results%>%arrange(desc(Accuracy))%>%head
plot(fit_rf2)
```

Results for **Data 2** with **rf** and **default bootstrap**:

```{r variable_importance_rf_bootstrap_Data 2}
varImp(fit_rf2)
```

```{r result_rf_bootstrap_Data 2, echo=FALSE}
tab_rf_boot2<-tibble(data="Data 2",model="rf_bootstrap",accuracy=results_rf2[1],recall_hernia=results_rf2[2],precision_hernia=results_rf2[3],F1_hernia=results_rf2[4],recall_spond=results_rf2[5],precision_spond=results_rf2[6],F1_spond=results_rf2[7])
tab_rf_boot2
```

### 3.4.4. Data 2 - 10-fold cross validation

```{r, include=FALSE}
set.seed(4867,sample.kind="Rounding")
```
```{r rf_cv_Data 2}
fit_rf_cv2<-train(class~degree_spondylolisthesis+pelvic_incidence+lumbar_lordosis_angle+sacral_slope+pelvic_radius+pelvic_tilt,
               method="rf",data=train_set2,tuneLength=5,trControl=trainControl(method="cv",number=10))
predictions_rf_cv2<-predict(fit_rf_cv2,newdata=test_set2,type="raw")
results_rf_cv2<-results_func2(predictions_rf_cv2,test_set2$class)
fit_rf_cv2$results%>%arrange(desc(Accuracy))%>%head
plot(fit_rf_cv2)
```

Results for **Data 2** with **rf** and **10-fold cross validation**:

```{r variable_importance_rf_cv_Data 2}
varImp(fit_rf_cv2)
```

```{r results_rf_cv_Data 2, echo=FALSE}
tab_rf_cv2<-tibble(data="Data 2",model="rf_cv",accuracy=results_rf_cv2[1],recall_hernia=results_rf_cv2[2],precision_hernia=results_rf_cv2[3],F1_hernia=results_rf_cv2[4],recall_spond=results_rf_cv2[5],precision_spond=results_rf_cv2[6],F1_spond=results_rf_cv2[7])
tab_rf_cv2
```


# 4. Analysis and Optimization

Results obtained from fitted models for both Data 1 and Data 2 can be summarized into the following tables:

```{r summary Data 1}
do.call("rbind",list(tab_benchmark1,tab_glm,tab_knn_boot1,tab_knn_cv1,tab_rf_boot1,tab_rf_cv1))
```

```{r summary Data 2}
do.call("rbind",list(tab_knn_boot2,tab_knn_cv2,tab_rf_boot2,tab_rf_cv2))
```

For Data 1, glm produces a model with the highest accuracy and recall which are prioritized metrics in the context of detecting abnormalities in observations.

Due to the small size of the datasets, training models using all predictors is computationally manageable. However, we can see in Section 2's correlation chart, that only 1 predictor is not strongly correlated to degree_spondylolisthesis. Therefore, in theory, degree_spondylolisthesis and pelvic_radius are enough to reasonably predict abnormalities.

```{r degree_spondylolisthesis vs pelvic_radius}
dat1%>%select(degree_spondylolisthesis,pelvic_radius,class)%>%filter(degree_spondylolisthesis<20)%>%
  ggplot(aes(degree_spondylolisthesis,pelvic_radius,color=class))+
  geom_point(alpha=0.4)
```

We can see from the plot that both outcomes are somehow clustered even when degree_spondylolisthesis is low. Higher pelvic_radius tends to indicate normal condition. Similar trend can be observed from combination of degree_spondylolisthesis and sacral_slope.

```{r degree_spondylolisthesis vs sacral_slope}
dat1%>%select(degree_spondylolisthesis,sacral_slope,class)%>%filter(degree_spondylolisthesis<20)%>%
  ggplot(aes(degree_spondylolisthesis,sacral_slope,color=class))+
  geom_point(alpha=0.4)
```

Unsurprisingly, random forest method confirms the importance of the three predictors in Data 1's models.